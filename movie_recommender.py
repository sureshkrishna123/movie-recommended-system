# -*- coding: utf-8 -*-
"""movie-recommender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YF6S6Uszc-bHphLeCD0_Tyz_-zudPs2A
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

movies = pd.read_csv("/content/drive/MyDrive/tmdb_5000_movies.csv")

credits = pd.read_csv("/content/drive/MyDrive/tmdb_5000_credits.csv")

movies.head(3)

credits.head(2)

movies.shape, credits.shape

credits['cast'].values

##  A huge dataset

print(movies.columns)

print("-----------------------")

credits.columns

## lets merge the 2 datasets

movies = movies.merge(credits,on='title')
movies.shape

movies['original_language'].value_counts()
## majority is english language

"""## columns which are important 

-> Budget cannot be a criteria for a recommender system

-> Genre is one of the most important one

-> Homepage : Not at all important

-> ID: because we need to create posters for the app

-> Keywords == Tags

-> original langauge: highly imbalanced dataset: So not good 

-> Title: since it will be always in english: Original title may have names in other languages

-> Overview : Same as Summary: so similar movies can be recommended, so important

-> Popularity: we are trying to create tags: so we wont use that

-> Overview and tagline are almost same: so just keep one

-> Cast: Depends on what actor is performing or not

-> CREW: Same goes for crew
"""

movies = movies[['movie_id','title','overview','genres','keywords','cast','crew']]

movies.head()

"""## What path to follow now

We want just 3 columns in our dataset. Movie id, name and tags

So for tags one, we will join rest of the columns in the genre column. Like combine all other tags in crew, cast.. column in genre column

The cast and all other columns are not in proper format, so lets handle that first
"""

## Lets check for the null values
    
movies.isnull().sum()

## only 3 entries in overview column
## lets drop those

movies.dropna(inplace=True)

## any values repeated

movies.duplicated().sum()

## no repeated values

movies.iloc[0]['genres']

## see it has 3 types of genres: Action, Adveture, Fantasy and Science Fiction

## genre is String of lists
## String of lists: lets convert it to Lists

import ast

ast.literal_eval('[{"id": 28, "name": "Action"}, {"id": 12, "name": "Adventure"}, {"id": 14, "name": "Fantasy"}, {"id": 878, "name": "Science Fiction"}]')

## see how it changed the thing

def convert(text):
    arr = []
    for i in ast.literal_eval(text):
        arr.append(i['name'])
    
    return arr

movies['genres'] = movies['genres'].apply(convert)
movies.head(1)

movies['keywords'] = movies['keywords'].apply(convert)
movies.head()

movies.iloc[0]['cast']

## has a huge dataset
## lets just take the top 3 actors and thats it

def convert_cast(text):
    arr= []
    count = 0
    for i in ast.literal_eval(text):
        if count<3:
            arr.append(i['name'])
            count+=1
    return arr

movies['cast'] = movies['cast'].apply(convert_cast)
movies.head(1)

movies['crew'][0]

## we will just use the director tag from this

def convert_crew(text):
    arr = []
    for i in ast.literal_eval(text):
        if i['job'] == 'Director':
            arr.append(i['name'])
    return arr

movies['crew'] = movies['crew'].apply(convert_crew)
movies.head(1)

movies['overview'][0]

## its in the string format, lets convert it to list

movies['overview'] = movies['overview'].apply(lambda x: x.split())
movies['overview'][0]

movies.head(1)

## names have gaps between them, so will be treated as 2 diff entities
## so lets replace space with undeescore

arr = ['cast','crew','genres','keywords']
for j in arr:
    movies[j] = movies[j].apply(lambda x: [i.replace(" ","_") for i in x])

movies.head(1)

movies['tags'] = movies['overview'] + movies['genres'] + movies['keywords'] + movies['cast'] + movies['crew']

new_df = movies[['movie_id','title','tags','genres']]
new_df.head(5)

## lets make the tags to string

new_df['tags'] = new_df['tags'].apply(lambda x: " ".join(x))
new_df.head(3)

new_df['tags'][0]

## its always recommended to put your text in lower case.
## so lets  convert to lower case

new_df['tags'] = new_df['tags'].apply(lambda x: x.lower())

## we need to stem the whole tags thing
## since dance same as dancing but treated as 2 diff entities

import nltk
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def stem(text):
    y = []
    
    for i in text.split():
        y.append(ps.stem(i))
        
    return " ".join(y)

new_df['tags'] = new_df['tags'].apply(stem)

new_df['tags'][0]

## marine changed to marin
## unique to uniqu

"""How to deal with this nlp text??

So what we will do is combine all the tags, find the most used 5000 words.

Then for each movie tag, see how many time the words are repeated among the 5000 most common, basically we are creating a bag of words.

In this way it will help us to club the same movies togethere
"""

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=5000,stop_words='english')

vector = cv.fit_transform(new_df['tags']).toarray()
vector.shape

## 4806 movies
## 5000 words

cv.get_feature_names()[:20]

## starting 20 most common words

len(cv.get_feature_names())

## since we are dealing in the higher dimesnions
## euclidia distance will fail as it wont be that reliable in higher dimensions
### simply we called this as curse of dimensionality

## so for that we will be using the cosine distance
## that is angle between the two vectors


## less the distance: more the similarity means less angle between two vectors


from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(vector)

## here we are taking distace(cosine angle) of each movie with otheer movie

cosine_similarity(vector).shape

## since 4806 movies: so 4806 distanced between one and rest of the movies

similarity = cosine_similarity(vector)

## this matrix will have diagonal as 1
## since similarity will be 1 of each movie with itself

## index of any movie
index = new_df[new_df['title']=='Batman Begins'].index[0]
print(index)

sorted(similarity[index],reverse=True)
## but index is not being shown

sorted(list(enumerate(similarity[0])),reverse=True, key = lambda x:x[1])
## index values also shown
## and also sorted on the basis of the second values and not on the index values

## recommendation function
## find the similarity vector in the similarity object for the particular movie
## and the 5 least distanced will be the movies similar to it

def recommend(movie):
    index = new_df[new_df['title']==movie].index[0]
    distance = sorted(list(enumerate(similarity[index])),reverse=True, key = lambda x:x[1])
    for i in distance[1:11]:
        print(new_df.iloc[i[0]].title)

recommend('Avatar')

recommend("The Dark Knight Rises")

